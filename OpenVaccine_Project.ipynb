{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OpenVaccine_Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcxZIrVNchoV",
        "outputId": "183eaba6-d1e5-4ee5-849e-75c575d36635"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_NRTBaRUtfF",
        "outputId": "4ac433e3-93e1-4d77-d68c-8b1bddc0424e"
      },
      "source": [
        "##### ##### LOADING DATA\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "BASE_FILE_PATH = \"/content/drive/MyDrive/DataScience/Project/data\"\n",
        "\n",
        "##### LOADING JSON, CONVERTING TO CSV\n",
        "TRAIN_DATA_PATH_JSON = BASE_FILE_PATH + \"/train.json\"\n",
        "TEST_DATA_PATH_JSON = BASE_FILE_PATH + \"/test.json\"\n",
        "\n",
        "train_data = pd.read_json(TRAIN_DATA_PATH_JSON, lines=True)\n",
        "test_data = pd.read_json(TEST_DATA_PATH_JSON, lines=True)\n",
        "\n",
        "print(\"Train data shape:\", train_data.shape)\n",
        "print(\"Test data shape:\", test_data.shape)\n",
        "\n",
        "print(\"Train data head:\\n\", train_data.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data shape: (2400, 19)\n",
            "Test data shape: (3634, 7)\n",
            "Train data head:\n",
            "    index  ...                                            deg_50C\n",
            "0      0  ...  [0.6382, 3.4773, 0.9988, 1.3228, 0.78770000000...\n",
            "1      1  ...  [7.6692, 0.0, 10.9561, 0.0, 0.0, 0.0, 0.0, 0.0...\n",
            "2      2  ...  [0.9501000000000001, 1.7974999999999999, 1.499...\n",
            "3      3  ...  [7.6692, -1.3223, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...\n",
            "4      4  ...  [0.0, 5.1198, -0.3551, -0.3518, 0.0, 0.0, 0.0,...\n",
            "\n",
            "[5 rows x 19 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQ_jK7KXct-y",
        "outputId": "8bb62354-289a-4061-fc14-7c7c08768e60"
      },
      "source": [
        "##### ##### PREPROCESSING DATA\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Assigning each unique character its own integer identifier\n",
        "# encoding_dict will be used in the models' embedding layers\n",
        "def convert_to_encoding(data_frame, encoding_dict):\n",
        "    cat_features = [\"sequence\", \"structure\", \"predicted_loop_type\"]\n",
        "    for feature in cat_features:\n",
        "        data_frame.loc[:, feature] = data_frame.loc[:, feature].apply(lambda row: [encoding_dict[x] for x in row])\n",
        "\n",
        "    return np.array(data_frame[cat_features].values.tolist())\n",
        "\n",
        "##### Splitting training data into training/validation sets\n",
        "train_data = train_data.loc[train_data.SN_filter == 1]\n",
        "train_data_shuffled = train_data.sample(frac=1)\n",
        "EIGHTY_PERCENT = int(len(train_data_shuffled) * 0.8)\n",
        "train_data = train_data_shuffled.iloc[:EIGHTY_PERCENT, :].reset_index(drop=True)\n",
        "valid_data = train_data_shuffled.iloc[EIGHTY_PERCENT:, :].reset_index(drop=True)\n",
        "\n",
        "##### Encoding each character of each string with convert_to_encoding()\n",
        "encoding_dict = {}\n",
        "data_chars = \"ACGU().BEHIMSX\"\n",
        "for i in range(len(data_chars)):\n",
        "    encoding_dict[data_chars[i]] = i\n",
        "\n",
        "##### Encoding and reshaping training, validation, training data\n",
        "output_cols = [\"reactivity\", \"deg_Mg_pH10\", \"deg_pH10\", \"deg_Mg_50C\", \"deg_50C\"]\n",
        "\n",
        "train_features = np.transpose(torch.Tensor(convert_to_encoding(train_data, encoding_dict)), (0, 2, 1))\n",
        "train_labels = np.transpose(torch.Tensor(np.array(train_data[output_cols].values.tolist())), (0, 2, 1))\n",
        "\n",
        "valid_features = np.transpose(torch.Tensor(convert_to_encoding(valid_data, encoding_dict)), (0, 2, 1))\n",
        "valid_labels = np.transpose(torch.Tensor(np.array(valid_data[output_cols].values.tolist())), (0, 2, 1))\n",
        "\n",
        "public_test_data = test_data.loc[test_data.seq_length == 107]\n",
        "private_test_data = test_data.loc[test_data.seq_length == 130]\n",
        "public_test_features = np.transpose(torch.Tensor(convert_to_encoding(public_test_data, encoding_dict)), (0, 2, 1))\n",
        "private_test_features = np.transpose(torch.Tensor(convert_to_encoding(private_test_data, encoding_dict)), (0, 2, 1))\n",
        "\n",
        "##### Sanity check\n",
        "print(\"train_features shape:\", train_features.shape)\n",
        "print(\"train_labels shape:\", train_labels.shape)\n",
        "print()\n",
        "print(\"valid_features shape:\", valid_features.shape)\n",
        "print(\"valid_labels shape:\", valid_labels.shape)\n",
        "print()\n",
        "print(\"public_test_features shape:\", public_test_features.shape)\n",
        "print(\"private_test_features shape:\", private_test_features.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_features shape: torch.Size([1271, 107, 3])\n",
            "train_labels shape: torch.Size([1271, 68, 5])\n",
            "\n",
            "valid_features shape: torch.Size([318, 107, 3])\n",
            "valid_labels shape: torch.Size([318, 68, 5])\n",
            "\n",
            "public_test_features shape: torch.Size([629, 107, 3])\n",
            "private_test_features shape: torch.Size([3005, 130, 3])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(ilocs[0], value)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mH6JyJJ1Bcc"
      },
      "source": [
        "##### ##### MODELS\n",
        "\n",
        "##### Simple LSTM model\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(len(encoding_dict), 150)\n",
        "        self.lstm = nn.LSTM(input_size=450, hidden_size=256, num_layers=2, bidirectional=True, batch_first=True, dropout=0.5)\n",
        "        self.linear = nn.Linear(512, 5)\n",
        "    \n",
        "    def forward(self, xb, num_scored):\n",
        "        # Passing input through embedding layer.\n",
        "        # Embedding layer adds new dimension, so we need to reshape\n",
        "        xb = self.embedding(xb)\n",
        "        xb = torch.reshape(xb, (-1, xb.shape[1], xb.shape[2] * xb.shape[3]))\n",
        "\n",
        "        xb, _ = self.lstm(xb)\n",
        "        xb = xb[:, :num_scored, :]\n",
        "        xb = self.linear(xb)\n",
        "\n",
        "        return xb\n",
        "\n",
        "##### LSTM + Convolutional model\n",
        "class LSTM_Alt(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(len(encoding_dict), 150)\n",
        "        self.lstm = nn.LSTM(input_size=450, hidden_size=256, num_layers=2, bidirectional=True, batch_first=True, dropout=0.5)\n",
        "        self.conv = nn.Conv1d(107, 107, 1)\n",
        "        self.linear = nn.Linear(512, 128)\n",
        "        self.linear2 = nn.Linear(128, 5)\n",
        "        # self.linear3 = nn.Linear(32, 5)\n",
        "    \n",
        "    def forward(self, xb, num_scored):\n",
        "        # Passing input through embedding layer.\n",
        "        # Embedding layer adds new dimension, so we need to reshape\n",
        "        xb = self.embedding(xb)\n",
        "        xb = torch.reshape(xb, (-1, xb.shape[1], xb.shape[2] * xb.shape[3]))\n",
        "\n",
        "        xb, _ = self.lstm(xb)\n",
        "        xb = self.conv(xb)\n",
        "        xb = xb[:, :num_scored, :]\n",
        "        xb = self.linear(xb)\n",
        "        xb = self.linear2(xb)\n",
        "\n",
        "        return xb\n",
        "\n",
        "##### Linear model (baseline)\n",
        "class Linear(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_dim, 5)\n",
        "    \n",
        "    def forward(self, xb, num_scored):\n",
        "        xb = xb[:, :num_scored, :]\n",
        "        xb = self.linear(xb)\n",
        "        return xb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJfwn0Fl2BqB"
      },
      "source": [
        "##### ##### LOSS FUNCTION\n",
        "\n",
        "class MCRMSE_Loss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, yhat, y):\n",
        "        mse = torch.mean(torch.square(yhat - y))\n",
        "        return torch.mean(torch.sqrt(mse))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvckFzOy3rdh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ce9d778-b657-4d1b-c3bf-3b9a464fc2d1"
      },
      "source": [
        "##### ##### TRAINING AND VALIDATION\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "##### HYPER PARAMETERS\n",
        "num_epochs = 45 \n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "device = 'cuda'\n",
        "criterion = MCRMSE_Loss()\n",
        "models = [LSTM(3).to(device), LSTM_Alt(3).to(device), Linear(3).to(device)]\n",
        "\n",
        "# From when I was doing one model at a time\n",
        "# model = LSTM(3).to(device)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)\n",
        "\n",
        "train_dataset = TensorDataset(train_features, train_labels)\n",
        "train_iter = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "valid_dataset = TensorDataset(valid_features, valid_labels)\n",
        "valid_iter = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "public_test_dataset = TensorDataset(public_test_features)\n",
        "private_test_dataset = TensorDataset(private_test_features)\n",
        "public_test_iter = DataLoader(public_test_dataset, shuffle=False)\n",
        "private_test_iter = DataLoader(private_test_dataset, shuffle=False)\n",
        "\n",
        "all_train_losses = []\n",
        "all_valid_losses = []\n",
        "\n",
        "for i in range(len(models)):\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "\n",
        "    model = models[i]\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_loss_batch = []\n",
        "\n",
        "        for X, y in train_iter:\n",
        "            if i == 2:\n",
        "                X, y = X.cuda(), y.cuda()\n",
        "            else:\n",
        "                X, y = X.to(torch.long).cuda(), y.cuda()\n",
        "\n",
        "            y_hat = model(X, 68)\n",
        "            loss = criterion(y_hat, y)\n",
        "            train_loss_batch.append(loss)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        train_loss = sum(train_loss_batch) / len(train_loss_batch)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        valid_loss = 0.0\n",
        "        valid_loss_batch = []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for X, y in valid_iter:\n",
        "                if i == 2:\n",
        "                    X, y = X.cuda(), y.cuda()\n",
        "                else:\n",
        "                    X, y = X.to(torch.long).cuda(), y.cuda()\n",
        "\n",
        "                y_hat = model(X, 68)\n",
        "                loss = criterion(y_hat, y)\n",
        "                valid_loss_batch.append(loss)\n",
        "        \n",
        "        valid_loss = sum(valid_loss_batch) / len(valid_loss_batch)\n",
        "        valid_losses.append(valid_loss)\n",
        "\n",
        "        print(\"Epoch \" + str(epoch) + \", train loss: \" + str(train_loss.item()))\n",
        "        print(\"Valid loss:\", str(valid_loss.item()))\n",
        "\n",
        "        # lr_scheduler.step()   # Did not end up using LR scheduler\n",
        "    \n",
        "    all_train_losses.append(train_losses)\n",
        "    all_valid_losses.append(valid_losses)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, train loss: 0.39667317271232605\n",
            "Valid loss: 0.33820122480392456\n",
            "Epoch 1, train loss: 0.32157668471336365\n",
            "Valid loss: 0.3118857443332672\n",
            "Epoch 2, train loss: 0.3044237196445465\n",
            "Valid loss: 0.2989290952682495\n",
            "Epoch 3, train loss: 0.29154106974601746\n",
            "Valid loss: 0.29065462946891785\n",
            "Epoch 4, train loss: 0.2823096215724945\n",
            "Valid loss: 0.28817030787467957\n",
            "Epoch 5, train loss: 0.272491455078125\n",
            "Valid loss: 0.27341270446777344\n",
            "Epoch 6, train loss: 0.26335713267326355\n",
            "Valid loss: 0.2676181495189667\n",
            "Epoch 7, train loss: 0.2576717138290405\n",
            "Valid loss: 0.2634415328502655\n",
            "Epoch 8, train loss: 0.25328925251960754\n",
            "Valid loss: 0.25916197896003723\n",
            "Epoch 9, train loss: 0.24735939502716064\n",
            "Valid loss: 0.2580049932003021\n",
            "Epoch 10, train loss: 0.2404182255268097\n",
            "Valid loss: 0.251766175031662\n",
            "Epoch 11, train loss: 0.2369290441274643\n",
            "Valid loss: 0.2511102855205536\n",
            "Epoch 12, train loss: 0.23179025948047638\n",
            "Valid loss: 0.24692487716674805\n",
            "Epoch 13, train loss: 0.2285417765378952\n",
            "Valid loss: 0.24770891666412354\n",
            "Epoch 14, train loss: 0.22482998669147491\n",
            "Valid loss: 0.24773739278316498\n",
            "Epoch 15, train loss: 0.22029085457324982\n",
            "Valid loss: 0.24523472785949707\n",
            "Epoch 16, train loss: 0.21923497319221497\n",
            "Valid loss: 0.24404297769069672\n",
            "Epoch 17, train loss: 0.2156904935836792\n",
            "Valid loss: 0.2412063628435135\n",
            "Epoch 18, train loss: 0.21129071712493896\n",
            "Valid loss: 0.24482570588588715\n",
            "Epoch 19, train loss: 0.20884950459003448\n",
            "Valid loss: 0.24182601273059845\n",
            "Epoch 20, train loss: 0.20593225955963135\n",
            "Valid loss: 0.24224554002285004\n",
            "Epoch 21, train loss: 0.2036774456501007\n",
            "Valid loss: 0.24259769916534424\n",
            "Epoch 22, train loss: 0.20025424659252167\n",
            "Valid loss: 0.23799167573451996\n",
            "Epoch 23, train loss: 0.19833509624004364\n",
            "Valid loss: 0.2397703379392624\n",
            "Epoch 24, train loss: 0.19488537311553955\n",
            "Valid loss: 0.2388436645269394\n",
            "Epoch 25, train loss: 0.19333326816558838\n",
            "Valid loss: 0.23814375698566437\n",
            "Epoch 26, train loss: 0.19048142433166504\n",
            "Valid loss: 0.24197712540626526\n",
            "Epoch 27, train loss: 0.18891048431396484\n",
            "Valid loss: 0.24404194951057434\n",
            "Epoch 28, train loss: 0.1878827065229416\n",
            "Valid loss: 0.2386031448841095\n",
            "Epoch 29, train loss: 0.18441693484783173\n",
            "Valid loss: 0.2377442568540573\n",
            "Epoch 30, train loss: 0.1826011836528778\n",
            "Valid loss: 0.23670747876167297\n",
            "Epoch 31, train loss: 0.1809217780828476\n",
            "Valid loss: 0.24169401824474335\n",
            "Epoch 32, train loss: 0.18084107339382172\n",
            "Valid loss: 0.23947027325630188\n",
            "Epoch 33, train loss: 0.17714373767375946\n",
            "Valid loss: 0.23579993844032288\n",
            "Epoch 34, train loss: 0.17541787028312683\n",
            "Valid loss: 0.23840223252773285\n",
            "Epoch 35, train loss: 0.17430607974529266\n",
            "Valid loss: 0.2365332692861557\n",
            "Epoch 36, train loss: 0.1723463386297226\n",
            "Valid loss: 0.23684529960155487\n",
            "Epoch 37, train loss: 0.17039118707180023\n",
            "Valid loss: 0.23627066612243652\n",
            "Epoch 38, train loss: 0.16969209909439087\n",
            "Valid loss: 0.23745094239711761\n",
            "Epoch 39, train loss: 0.16812549531459808\n",
            "Valid loss: 0.2352682650089264\n",
            "Epoch 40, train loss: 0.1668284684419632\n",
            "Valid loss: 0.23589253425598145\n",
            "Epoch 41, train loss: 0.16628512740135193\n",
            "Valid loss: 0.2353833168745041\n",
            "Epoch 42, train loss: 0.16459307074546814\n",
            "Valid loss: 0.2361517995595932\n",
            "Epoch 43, train loss: 0.1626044064760208\n",
            "Valid loss: 0.23691247403621674\n",
            "Epoch 44, train loss: 0.16139338910579681\n",
            "Valid loss: 0.23837855458259583\n",
            "Epoch 0, train loss: 0.4720516800880432\n",
            "Valid loss: 0.41577085852622986\n",
            "Epoch 1, train loss: 0.404241144657135\n",
            "Valid loss: 0.4075455665588379\n",
            "Epoch 2, train loss: 0.39558205008506775\n",
            "Valid loss: 0.401224285364151\n",
            "Epoch 3, train loss: 0.38774698972702026\n",
            "Valid loss: 0.3954051733016968\n",
            "Epoch 4, train loss: 0.3818280100822449\n",
            "Valid loss: 0.3899848163127899\n",
            "Epoch 5, train loss: 0.3734264075756073\n",
            "Valid loss: 0.37873658537864685\n",
            "Epoch 6, train loss: 0.3614734709262848\n",
            "Valid loss: 0.3714771568775177\n",
            "Epoch 7, train loss: 0.35040077567100525\n",
            "Valid loss: 0.35900670289993286\n",
            "Epoch 8, train loss: 0.33979153633117676\n",
            "Valid loss: 0.34727057814598083\n",
            "Epoch 9, train loss: 0.329332172870636\n",
            "Valid loss: 0.3406025767326355\n",
            "Epoch 10, train loss: 0.3203757107257843\n",
            "Valid loss: 0.3333512246608734\n",
            "Epoch 11, train loss: 0.312857449054718\n",
            "Valid loss: 0.32831329107284546\n",
            "Epoch 12, train loss: 0.3058593273162842\n",
            "Valid loss: 0.32484397292137146\n",
            "Epoch 13, train loss: 0.3003076910972595\n",
            "Valid loss: 0.32330262660980225\n",
            "Epoch 14, train loss: 0.2945444881916046\n",
            "Valid loss: 0.3154087960720062\n",
            "Epoch 15, train loss: 0.2880246341228485\n",
            "Valid loss: 0.313615620136261\n",
            "Epoch 16, train loss: 0.2843645215034485\n",
            "Valid loss: 0.3117665648460388\n",
            "Epoch 17, train loss: 0.27870216965675354\n",
            "Valid loss: 0.30879491567611694\n",
            "Epoch 18, train loss: 0.27431634068489075\n",
            "Valid loss: 0.30568060278892517\n",
            "Epoch 19, train loss: 0.26922082901000977\n",
            "Valid loss: 0.3032167851924896\n",
            "Epoch 20, train loss: 0.26595422625541687\n",
            "Valid loss: 0.3041010797023773\n",
            "Epoch 21, train loss: 0.2629829943180084\n",
            "Valid loss: 0.3009396195411682\n",
            "Epoch 22, train loss: 0.25851157307624817\n",
            "Valid loss: 0.29740604758262634\n",
            "Epoch 23, train loss: 0.25412335991859436\n",
            "Valid loss: 0.2972356975078583\n",
            "Epoch 24, train loss: 0.25069460272789\n",
            "Valid loss: 0.29693207144737244\n",
            "Epoch 25, train loss: 0.24714748561382294\n",
            "Valid loss: 0.29464900493621826\n",
            "Epoch 26, train loss: 0.24374954402446747\n",
            "Valid loss: 0.2948143482208252\n",
            "Epoch 27, train loss: 0.24080733954906464\n",
            "Valid loss: 0.2911596894264221\n",
            "Epoch 28, train loss: 0.2364589273929596\n",
            "Valid loss: 0.29121556878089905\n",
            "Epoch 29, train loss: 0.2336152046918869\n",
            "Valid loss: 0.2884543836116791\n",
            "Epoch 30, train loss: 0.2303164005279541\n",
            "Valid loss: 0.2886766195297241\n",
            "Epoch 31, train loss: 0.2277727574110031\n",
            "Valid loss: 0.28774315118789673\n",
            "Epoch 32, train loss: 0.22465479373931885\n",
            "Valid loss: 0.2855534255504608\n",
            "Epoch 33, train loss: 0.22178266942501068\n",
            "Valid loss: 0.2850697934627533\n",
            "Epoch 34, train loss: 0.21914315223693848\n",
            "Valid loss: 0.28404098749160767\n",
            "Epoch 35, train loss: 0.21674001216888428\n",
            "Valid loss: 0.28235968947410583\n",
            "Epoch 36, train loss: 0.21470732986927032\n",
            "Valid loss: 0.28014037013053894\n",
            "Epoch 37, train loss: 0.21108503639698029\n",
            "Valid loss: 0.2796187400817871\n",
            "Epoch 38, train loss: 0.20841479301452637\n",
            "Valid loss: 0.27829816937446594\n",
            "Epoch 39, train loss: 0.2064884901046753\n",
            "Valid loss: 0.2782995104789734\n",
            "Epoch 40, train loss: 0.20391301810741425\n",
            "Valid loss: 0.2778131663799286\n",
            "Epoch 41, train loss: 0.20181047916412354\n",
            "Valid loss: 0.2752549648284912\n",
            "Epoch 42, train loss: 0.19959358870983124\n",
            "Valid loss: 0.27534815669059753\n",
            "Epoch 43, train loss: 0.19708983600139618\n",
            "Valid loss: 0.27414968609809875\n",
            "Epoch 44, train loss: 0.1954725831747055\n",
            "Valid loss: 0.27414703369140625\n",
            "Epoch 0, train loss: 2.6232964992523193\n",
            "Valid loss: 2.3577420711517334\n",
            "Epoch 1, train loss: 2.095813751220703\n",
            "Valid loss: 1.847605586051941\n",
            "Epoch 2, train loss: 1.6103315353393555\n",
            "Valid loss: 1.3911268711090088\n",
            "Epoch 3, train loss: 1.2007527351379395\n",
            "Valid loss: 1.0408821105957031\n",
            "Epoch 4, train loss: 0.928959846496582\n",
            "Valid loss: 0.8528165817260742\n",
            "Epoch 5, train loss: 0.8097677230834961\n",
            "Valid loss: 0.7884442210197449\n",
            "Epoch 6, train loss: 0.7707694172859192\n",
            "Valid loss: 0.7646259069442749\n",
            "Epoch 7, train loss: 0.7522591948509216\n",
            "Valid loss: 0.7480718493461609\n",
            "Epoch 8, train loss: 0.7369663715362549\n",
            "Valid loss: 0.7332417368888855\n",
            "Epoch 9, train loss: 0.7218558192253113\n",
            "Valid loss: 0.7178822159767151\n",
            "Epoch 10, train loss: 0.7071515321731567\n",
            "Valid loss: 0.703209400177002\n",
            "Epoch 11, train loss: 0.6925546526908875\n",
            "Valid loss: 0.6886961460113525\n",
            "Epoch 12, train loss: 0.6784887909889221\n",
            "Valid loss: 0.6744110584259033\n",
            "Epoch 13, train loss: 0.6641594767570496\n",
            "Valid loss: 0.6604424715042114\n",
            "Epoch 14, train loss: 0.650582492351532\n",
            "Valid loss: 0.6468328833580017\n",
            "Epoch 15, train loss: 0.6369662284851074\n",
            "Valid loss: 0.633392870426178\n",
            "Epoch 16, train loss: 0.623516321182251\n",
            "Valid loss: 0.6203243732452393\n",
            "Epoch 17, train loss: 0.6113130450248718\n",
            "Valid loss: 0.6077022552490234\n",
            "Epoch 18, train loss: 0.5984681248664856\n",
            "Valid loss: 0.5955591797828674\n",
            "Epoch 19, train loss: 0.5867709517478943\n",
            "Valid loss: 0.5838302969932556\n",
            "Epoch 20, train loss: 0.5752224326133728\n",
            "Valid loss: 0.5726667046546936\n",
            "Epoch 21, train loss: 0.5645714998245239\n",
            "Valid loss: 0.562035858631134\n",
            "Epoch 22, train loss: 0.5542489290237427\n",
            "Valid loss: 0.5519228577613831\n",
            "Epoch 23, train loss: 0.5441314578056335\n",
            "Valid loss: 0.5425410866737366\n",
            "Epoch 24, train loss: 0.5348186492919922\n",
            "Valid loss: 0.5338506102561951\n",
            "Epoch 25, train loss: 0.526422917842865\n",
            "Valid loss: 0.5252265930175781\n",
            "Epoch 26, train loss: 0.5182719826698303\n",
            "Valid loss: 0.5174505114555359\n",
            "Epoch 27, train loss: 0.5110152959823608\n",
            "Valid loss: 0.5102599263191223\n",
            "Epoch 28, train loss: 0.50405353307724\n",
            "Valid loss: 0.5038021206855774\n",
            "Epoch 29, train loss: 0.49791303277015686\n",
            "Valid loss: 0.4978425204753876\n",
            "Epoch 30, train loss: 0.49234578013420105\n",
            "Valid loss: 0.4925149977207184\n",
            "Epoch 31, train loss: 0.48724961280822754\n",
            "Valid loss: 0.4877285659313202\n",
            "Epoch 32, train loss: 0.4826938807964325\n",
            "Valid loss: 0.48354440927505493\n",
            "Epoch 33, train loss: 0.47838082909584045\n",
            "Valid loss: 0.4796602427959442\n",
            "Epoch 34, train loss: 0.47493648529052734\n",
            "Valid loss: 0.47652074694633484\n",
            "Epoch 35, train loss: 0.4717753529548645\n",
            "Valid loss: 0.47333288192749023\n",
            "Epoch 36, train loss: 0.46887484192848206\n",
            "Valid loss: 0.47098293900489807\n",
            "Epoch 37, train loss: 0.46641477942466736\n",
            "Valid loss: 0.46868354082107544\n",
            "Epoch 38, train loss: 0.4645141065120697\n",
            "Valid loss: 0.4667057693004608\n",
            "Epoch 39, train loss: 0.46269965171813965\n",
            "Valid loss: 0.4653187692165375\n",
            "Epoch 40, train loss: 0.46142950654029846\n",
            "Valid loss: 0.46380558609962463\n",
            "Epoch 41, train loss: 0.46012696623802185\n",
            "Valid loss: 0.46270713210105896\n",
            "Epoch 42, train loss: 0.4588780403137207\n",
            "Valid loss: 0.4616251587867737\n",
            "Epoch 43, train loss: 0.45821332931518555\n",
            "Valid loss: 0.46112948656082153\n",
            "Epoch 44, train loss: 0.45745354890823364\n",
            "Valid loss: 0.4601503312587738\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "e3Vxy1nFmZf6",
        "outputId": "863dcefe-7b20-44d3-ce85-249835a93535"
      },
      "source": [
        "##### ##### VISUALIZING LOSS OVER TIME\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "epochs_list = range(num_epochs)\n",
        "# plt.plot(epochs_list, train_losses, 'bo-', label='Training Loss')     # Not showing training loss\n",
        "plt.plot(epochs_list, all_valid_losses[0], 'r', label='Simple LSTM Validation Loss')\n",
        "plt.plot(epochs_list, all_valid_losses[1], 'g', label='LSTM + Conv Validation Loss')\n",
        "plt.plot(epochs_list, all_valid_losses[2], 'b', label='Linear (Baseline) Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-jurWjx8eyQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f38f721-d1c7-4644-e92a-47a3dc47e9a6"
      },
      "source": [
        "##### ##### TESTING\n",
        "public_preds = []\n",
        "private_preds = []\n",
        "model = models[0]\n",
        "\n",
        "public_preds = np.zeros((0, 107, 5))\n",
        "private_preds = np.zeros((0, 130, 5))\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for X in public_test_iter:\n",
        "        X = X[0].to(torch.int64).cuda()\n",
        "        pred = model(X, 107)\n",
        "\n",
        "        public_preds = np.concatenate([public_preds, pred.cpu()], axis=0)\n",
        "    \n",
        "    for X in private_test_iter:\n",
        "        X = X[0].to(torch.int64).cuda()\n",
        "        pred = model(X, 130)\n",
        "\n",
        "        private_preds = np.concatenate([private_preds, pred.cpu()], axis=0)\n",
        "\n",
        "print(\"public_preds shape:\", public_preds.shape)\n",
        "print(\"private_preds shape:\", private_preds.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "public_preds shape: (629, 107, 5)\n",
            "private_preds shape: (3005, 130, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5CV7DG8JK7F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b48e52e6-fffc-4f84-a94e-f02b94f373a4"
      },
      "source": [
        "##### ##### COMBINING OUTPUT PREDICTIONS \n",
        "i = 0\n",
        "public_submission = np.zeros((0, 6))\n",
        "public_submission = pd.DataFrame(public_submission, columns=[\"id_seqpos\", \"reactivity\", \"deg_Mg_pH10\", \"deg_pH10\", \"deg_Mg_50C\", \"deg_50C\"])\n",
        "\n",
        "for id in public_test_data[\"id\"]:\n",
        "    submission_batch = pd.DataFrame(public_preds[i], columns=output_cols)\n",
        "    id_list = []\n",
        "\n",
        "    for j in range(public_preds[i].shape[0]):\n",
        "        id_list.append(str(id) + \"_\" + str(j) )\n",
        "    submission_batch[\"id_seqpos\"] = id_list\n",
        "\n",
        "    public_submission = pd.concat([public_submission, submission_batch])\n",
        "\n",
        "    i += 1\n",
        "\n",
        "private_submission = np.zeros((0, 6))\n",
        "private_submission = pd.DataFrame(private_submission, columns=[\"id_seqpos\", \"reactivity\", \"deg_Mg_pH10\", \"deg_pH10\", \"deg_Mg_50C\", \"deg_50C\"])\n",
        "\n",
        "i = 0\n",
        "for id in private_test_data[\"id\"]:\n",
        "    submission_batch = pd.DataFrame(private_preds[i], columns=output_cols)\n",
        "    id_list = []\n",
        "\n",
        "    for j in range(private_preds[i].shape[0]):\n",
        "        id_list.append(str(id) + \"_\" + str(j) )\n",
        "    submission_batch[\"id_seqpos\"] = id_list\n",
        "\n",
        "    private_submission = pd.concat([private_submission, submission_batch])\n",
        "\n",
        "    i += 1\n",
        "\n",
        "print(\"public_submission shape:\", public_submission.shape)\n",
        "print(\"private_submission shape:\", private_submission.shape)\n",
        "\n",
        "submission = pd.concat([public_submission, private_submission])\n",
        "submission = submission.reset_index(drop=True)\n",
        "\n",
        "print()\n",
        "print(\"submission shape:\", submission.shape)\n",
        "print(\"submission head:\", submission.head())\n",
        "     "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "public_submission shape: (67303, 6)\n",
            "private_submission shape: (390650, 6)\n",
            "\n",
            "submission shape: (457953, 6)\n",
            "submission head:         id_seqpos  reactivity  deg_Mg_pH10  deg_pH10  deg_Mg_50C   deg_50C\n",
            "0  id_00073f8be_0    1.009932     0.748724  1.922320    0.691993  0.815671\n",
            "1  id_00073f8be_1    3.061346     3.559828  4.553716    3.647369  3.038272\n",
            "2  id_00073f8be_2    1.798041     0.647612  0.746818    0.707006  0.791971\n",
            "3  id_00073f8be_3    1.630163     1.461275  1.439296    1.985584  1.908636\n",
            "4  id_00073f8be_4    1.065874     0.735369  0.809032    1.082201  1.070072\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW3QbHrO9QCl"
      },
      "source": [
        "##### ##### SAVING TO .CSV\n",
        "submission.to_csv(\"/content/drive/MyDrive/DataScience/Project/submission.csv\", index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}